# ğŸ‘©â€ğŸ« TIA â€” Tutor de InteligÃªncia Artificial

TIA (Tutor de InteligÃªncia Artificial) Ã© um sistema de apoio ao estudo baseado em IA, capaz de responder perguntas dos estudantes utilizando o conteÃºdo de PDFs enviados pelos prÃ³prios usuÃ¡rios. O sistema combina tÃ©cnicas de RAG (Retrieval-Augmented Generation) e pode operar totalmente offline (usando modelos locais via Ollama) ou online (usando o Google Gemini, se disponÃ­vel).

---

## âœ¨ Funcionalidades

- **Chat com histÃ³rico e mÃºltiplas conversas**
- **Upload de PDFs:** expanda a base de conhecimento facilmente pela interface web
- **Respostas pedagÃ³gicas:** o tutor nÃ£o entrega respostas prontas, mas utiliza analogias, exemplos do cotidiano e perguntas-guia para incentivar o raciocÃ­nio e a autonomia do estudante
- **Base de conhecimento local:** seus arquivos e dados permanecem em sua mÃ¡quina
- **Failover automÃ¡tico:** utiliza Gemini (online) se disponÃ­vel, ou Ollama (offline) se nÃ£o houver conexÃ£o
- **Pipeline RAG:** busca trechos relevantes dos PDFs para dar contexto Ã s respostas

---

> ğŸ’¡ **Nota pedagÃ³gica**  
> O TIA foi projetado para **nÃ£o dar respostas diretas**. Seu objetivo Ã© atuar como um tutor, promovendo a reflexÃ£o e a construÃ§Ã£o autÃ´noma do conhecimento, usando exemplos, analogias e perguntas orientadoras.

---

## ğŸ—ï¸ Arquitetura

- **Frontend:** Streamlit (interface web)
- **Backend:** FastAPI (API para chat e upload)
- **Pipeline de IngestÃ£o:** Processamento de PDFs, geraÃ§Ã£o de embeddings com Ollama e armazenamento no ChromaDB
- **Pipeline RAG:** RecuperaÃ§Ã£o de contexto dos PDFs, montagem de prompt pedagÃ³gico e resposta via LLM (Gemini ou Ollama)

---

## ğŸš€ Como usar

1. **Instale as dependÃªncias**
   ```bash
   pip install -r requirements.txt
   ```

2. **Baixe e rode o Ollama**
   - Instale o [Ollama](https://ollama.com/)
   - Baixe os modelos necessÃ¡rios:
     ```bash
     ollama pull nomic-embed-text
     ollama pull gemma3:1b
     ```

3. **(Opcional) Configure o Gemini**
   - Crie um arquivo `.env` com a chave da API Gemini:
     ```
     GOOGLE_API_KEY=sua_chave_aqui
     ```

4. **Execute a ingestÃ£o inicial (opcional)**
   - Coloque PDFs em `./knowledge_base/` e rode:
     ```bash
     python ingest.py
     ```

5. **Inicie os servidores**
   - Backend (FastAPI):
     ```bash
     uvicorn main:app --reload
     ```
   - Frontend (Streamlit):
     ```bash
     streamlit run app.py
     ```
   - O frontend estarÃ¡ em [http://localhost:8501](http://localhost:8501)
   - O backend FastAPI estarÃ¡ em [http://localhost:8000](http://localhost:8000)

---

## ğŸ“‚ Estrutura do Projeto

```
/project-root
â”‚
â”œâ”€â”€ app.py                # Frontend Streamlit
â”œâ”€â”€ main.py               # Backend FastAPI
â”œâ”€â”€ ingest.py             # Pipeline de ingestÃ£o de PDFs
â”œâ”€â”€ rag_pipeline.py       # Pipeline de RAG hÃ­brido
â”œâ”€â”€ knowledge_base/       # PDFs enviados pelos usuÃ¡rios
â”œâ”€â”€ vector_db/            # Banco vetorial persistente (ChromaDB)
â”œâ”€â”€ .env                  # (Opcional) API Key do Gemini
â””â”€â”€ requirements.txt      # DependÃªncias
```

---

## âš™ï¸ Tecnologias Utilizadas

- **Python 3.10+**
- **Streamlit** (interface web)
- **FastAPI** (API backend)
- **Ollama** (LLM/embeddings local)
- **ChromaDB** (banco vetorial)
- **LangChain Community**
- **Google Generative AI (Gemini)**

---

## ğŸ”’ Privacidade & SeguranÃ§a

- Todos os arquivos e embeddings ficam locais (offline), exceto ao usar Gemini.
- Nenhum dado do usuÃ¡rio Ã© enviado para terceiros, exceto quando a rota Gemini Ã© utilizada.
- Recomenda-se rodar o sistema apenas em ambientes controlados.

---

## ğŸ› ï¸ Problemas comuns

- **Respostas curtas ou sem contexto:** certifique-se de que PDFs jÃ¡ foram processados.
- **Gemini nÃ£o funciona:** verifique conectividade e chave da API.
- **Ollama nÃ£o responde/erro de modelo:** certifique-se de que o Ollama estÃ¡ rodando e que os modelos estÃ£o baixados.

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob os termos da [LicenÃ§a MIT](LICENSE).  
Consulte as licenÃ§as dos modelos Ollama e Gemini para uso comercial.

---

## ğŸ¤ ContribuiÃ§Ã£o

SugestÃµes, correÃ§Ãµes e melhorias sÃ£o bem-vindas! Abra uma issue ou um pull request.
