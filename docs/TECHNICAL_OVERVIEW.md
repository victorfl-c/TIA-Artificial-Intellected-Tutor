# üë©‚Äçüè´ TIA ‚Äî Documenta√ß√£o T√©cnica

## Vis√£o Geral

TIA (Tutor de Intelig√™ncia Artificial) √© um sistema de apoio ao estudo baseado em IA, capaz de responder perguntas dos estudantes utilizando o conte√∫do de PDFs enviados pelos pr√≥prios usu√°rios. O sistema combina t√©cnicas de RAG (Retrieval-Augmented Generation), mantendo flexibilidade para operar totalmente offline (usando modelos locais via Ollama) ou online (usando o Google Gemini se dispon√≠vel).

> üí° **Nota pedag√≥gica:**  
> O TIA foi projetado para **n√£o dar respostas diretas**. Seu objetivo √© atuar como um tutor, promovendo a reflex√£o e a constru√ß√£o aut√¥noma do conhecimento pelo estudante atrav√©s de exemplos, analogias e perguntas orientadoras.

---

## üèóÔ∏è Arquitetura Detalhada

O sistema √© composto por um frontend interativo, um backend que orquestra a l√≥gica, um pipeline de ingest√£o de dados e o pipeline RAG h√≠brido que gera as respostas.

### 1. Frontend (`app.py`)
- **Framework:** Streamlit.
- **Fun√ß√µes principais:**
  - Interface de chat com hist√≥rico e suporte a m√∫ltiplas conversas.
  - Funcionalidade de upload de arquivos PDF para alimentar a base de conhecimento.
  - Comunica√ß√£o com o backend FastAPI para enviar perguntas e arquivos.
  - Gerenciamento de estado: As conversas e seus t√≠tulos s√£o mantidos no `st.session_state` do Streamlit.

### 2. Backend API (`main.py`)
- **Framework:** FastAPI.
- **Principais endpoints:**
  - `/ask-stream`: Recebe a pergunta e o hist√≥rico da conversa, retornando a resposta do tutor em formato de streaming para uma experi√™ncia interativa.
  - `/upload`: Gerencia o upload de m√∫ltiplos arquivos PDF, salvando-os na pasta `./knowledge_base/` e acionando o processo de reingest√£o da base de conhecimento.

### 3. Pipeline de Ingest√£o (`ingest.py`)
- **Fun√ß√£o principal:** `run_ingestion()`.
- **Opera√ß√£o:**
  - Carrega documentos PDF da pasta `./knowledge_base/`.
  - Divide os documentos em chunks (fragmentos de texto) menores e com sobreposi√ß√£o, utilizando o `RecursiveCharacterTextSplitter` do LangChain.
  - Gera embeddings (vetores num√©ricos) para cada chunk localmente, usando o modelo `nomic-embed-text` via Ollama.
  - Armazena os vetores e os textos correspondentes em um banco de dados vetorial local e persistente (ChromaDB).
  - **Para evitar duplicatas:** Antes de adicionar os novos chunks de um PDF, o script remove todos os chunks antigos daquele arquivo (com base no campo `source` nos metadados).

### 4. Pipeline RAG H√≠brido (`rag_pipeline.py`)
- **Fluxo principal:** `get_hybrid_response_stream(question, history)`.
- **Opera√ß√£o:**
  - Gera um embedding para a pergunta do usu√°rio via Ollama.
  - Consulta o ChromaDB para recuperar os chunks de texto mais relevantes (Top-K) que servir√£o de contexto.
  - Monta um prompt detalhado contendo o contexto recuperado, o hist√≥rico recente da conversa e a pergunta do usu√°rio. O prompt √© constru√≠do pela fun√ß√£o `build_tutor_prompt`, que instrui o modelo a agir como um tutor e n√£o a dar respostas diretas.
  - **L√≥gica de Failover:** O sistema verifica a conectividade com a internet. Se houver conex√£o, tenta gerar a resposta com o modelo online (Google Gemini). Em caso de falha ou aus√™ncia de conex√£o, aciona o modelo local (Ollama) automaticamente.

---

## üöÄ Como Usar (Instala√ß√£o e Execu√ß√£o)

1. **Instale as depend√™ncias**  
   ```bash
   pip install -r requirements.txt
   ```

2. **Baixe e rode o Ollama**  
   - Instale o [Ollama](https://ollama.com/) em sua m√°quina.
   - Baixe os modelos necess√°rios via terminal:
     ```bash
     ollama pull nomic-embed-text
     ollama pull gemma3:1b
     ```

3. **(Opcional) Configure o Google Gemini**  
   - Crie um arquivo chamado `.env` na raiz do projeto.
   - Adicione sua chave da API do Gemini ao arquivo:
     ```
     GOOGLE_API_KEY=sua_chave_aqui
     ```

4. **Execute a ingest√£o de dados (opcional)**  
   - Para popular a base de conhecimento inicial, coloque arquivos PDF na pasta `./knowledge_base/`.
   - Execute o script de ingest√£o:
     ```bash
     python ingest.py
     ```

5. **Inicie os servidores**  
   - Inicie o servidor de backend (FastAPI) em um terminal:
     ```bash
     uvicorn main:app --reload
     ```
   - Inicie o servidor de frontend (Streamlit) em outro terminal:
     ```bash
     streamlit run app.py
     ```
   - Acesse a interface do TIA no seu navegador em [http://localhost:8501](http://localhost:8501).

---

## üìÇ Estrutura de Pastas Recomendada

```
/project-root
‚îÇ
‚îú‚îÄ‚îÄ app.py                # Frontend Streamlit
‚îú‚îÄ‚îÄ main.py               # Backend FastAPI
‚îú‚îÄ‚îÄ ingest.py             # Pipeline de ingest√£o de PDFs
‚îú‚îÄ‚îÄ rag_pipeline.py       # Pipeline de RAG h√≠brido
‚îú‚îÄ‚îÄ knowledge_base/       # PDFs enviados pelos usu√°rios
‚îú‚îÄ‚îÄ vector_db/            # Banco vetorial persistente do ChromaDB
‚îú‚îÄ‚îÄ .env                  # Vari√°veis de ambiente (ex: API Key do Gemini)
‚îî‚îÄ‚îÄ requirements.txt      # Depend√™ncias do projeto
```

---

## ‚öôÔ∏è Tecnologias Utilizadas

- **Python 3.10+**
- **streamlit:** Interface web do usu√°rio
- **fastapi / uvicorn:** API backend
- **chromadb:** Armazenamento vetorial
- **langchain-community / langchain-text-splitters:** Carregamento e chunking de PDFs
- **google-generativeai:** Gemini LLM online
- **ollama:** LLM e embeddings locais
- **python-dotenv:** Carregamento de vari√°veis de ambiente
- **requests:** Comunica√ß√£o frontend-backend

---

## Pontos T√©cnicos Relevantes

- **ChromaDB:** Armazena embeddings de chunks de texto dos PDFs, com persist√™ncia local.
- **Ollama:** Usado tanto para gerar embeddings quanto para respostas offline via LLM local.
- **Google Gemini:** Usado para respostas online, se chave e conex√£o dispon√≠veis.
- **Prompt √önico:** Fun√ß√£o `build_tutor_prompt` garante que tanto Gemini quanto Ollama recebem o mesmo contexto e instru√ß√£o.
- **Hist√≥rico de conversa:** Enviado junto √† pergunta, permitindo que o tutor LLM mantenha contexto e continuidade pedag√≥gica.
- **Failover autom√°tico:** Se Gemini n√£o estiver dispon√≠vel ou falhar, o sistema cai automaticamente para Ollama.
- **Logging:** Toda a aplica√ß√£o usa logging padronizado para f√°cil diagn√≥stico.
- **Evita duplicatas na base:** Antes de adicionar chunks de um PDF, remove todos os chunks antigos daquele arquivo na cole√ß√£o, considerando o campo `source` do metadata.

---

## Observa√ß√µes Finais

- O sistema foi desenhado para ser extens√≠vel e seguro: os PDFs e embeddings s√£o mantidos localmente, a menos que Gemini seja usado.
- O prompt pedag√≥gico pode ser facilmente customizado na fun√ß√£o `build_tutor_prompt`.
- O hist√≥rico das conversas √© mantido apenas na sess√£o do usu√°rio (client-side), sem persist√™ncia de longo prazo.
- Se desejar autentica√ß√£o, logging avan√ßado, ou persist√™ncia de hist√≥rico, adapte os m√≥dulos correspondentes.

---

## üìÑ Licen√ßa

Este projeto est√° licenciado sob os termos da Licen√ßa MIT. Consulte o arquivo [LICENSE](https://github.com/victorfl-c/TIA-Artificial-Intellected-Tutor/blob/main/LICENSE) para mais detalhes.
